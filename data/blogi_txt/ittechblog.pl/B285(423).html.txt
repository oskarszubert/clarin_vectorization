Tomasz Fiedoruk    - Właściciel serwisu - Business Strategy Development Expert w kilku firmach i w życiu. Ewangelista nowych technologii i innowacji.   @iTomek   Email Tomasz Fiedoruk, 29 stycznia 2009, 8:30   25   Nie daj się, stosuj robots.txt Ostatnio wziąłem się za analizę ruchu wyszukiwarek co nieuchronnie prowadzi do konfiguracji w pliku robots.txt. Otworzyłem plik /robots.txt z kilku największych blogów w Polsce … i okazało się, że nie mają nic skonfigurowanego. Mediafun nawet nie ma w ogóle pliku na serwerze. Antyweb ma domyślny wpis. I tak można by jeszcze trochę powymieniać ;) Z polskiej czołówki pozytywnie wyróżnia się webfan.pl i tomasz.topa.pl oraz oczywiście wpninja.pl. Dlaczego w ogóle wziąłem się za ten temat? Dzisiaj analizowałem swojego bloga i działanie robotów na stronie Narzędzi Google dla Webmasterów i zauważyłem, że mam bardzo restrykcyjny plik robots.txt, który wyglądał mniej więcej tak: # BEGIN XML- SITEMAP-PLUGIN Sitemap: http://ittechblog.pl/sitemap.xml.gz # END XML-SITEMAP-PLUGIN User-agent: * # disallow files in /cgi- bin Disallow: /cgi-bin/ Disallow: /comments/ Disallow: /z/j/ Disallow: /z/c/ # disallow all files ending in .php Disallow: /*.php$ Disallow: /*.js$ Disallow: /*.inc$ Disallow: /*.css$ Disallow: /*.txt$ #disallow all files in /wp- directorys Disallow: /wp-*/ # disallow all files with ? in url Disallow: /*? # disallow any files that are stats related Disallow: /stats* Disallow: /about/legal-notice/ Disallow: /about /copyright-policy/ Disallow: /about/terms-and-conditions/ Disallow: /about/feed/ Disallow: /about/trackback/ Disallow: /contact/ Disallow: /tag Disallow: /docs* Disallow: /manual* Disallow: /category/uncategorized* Obecnie mam bardziej liberalny, co możecie sami sprawdzić :) Co dziwne, wiele blogów traktujących o SEO, pozycjonowaniu czy optymalizacji WP i ogólnie stron nie ma zdefiniowanego w ogóle pliku robots.txt. Zaznaczam, że nie jest to wpis mający na celu “wypunktować” braki w innych blogach a jedynie ma nakreślić obszary do poprawy. Dobry plik robots.txt ułatwi i nam życie (ograniczy ruch i obciążenie serwera, zabezpieczy materiały poufne, zwiększy bezpieczeńśtwo skryptu) i wyszukiwarkom (ułatwiając im życie poprawiamy im “humor” co może się odbić w naszych pozycjach w wynikach wyszukiwania). O samym robots.txt czy warto i dlaczego poczytajcie (dość stary ale nadal aktualny) wpis http://webfan.pl/robots-txt.html i na zakąskę wpis o SEO na sprawnymarketing.pl. Macie jakieś inne propozycje, co powinien zawierać idealny plik robots.txt? 